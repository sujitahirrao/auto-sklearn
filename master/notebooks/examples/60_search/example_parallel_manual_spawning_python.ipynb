{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Parallel Usage: Spawning workers from within Python\n\n*Auto-sklearn* uses\n`dask.distributed <https://distributed.dask.org/en/latest/index.html>`_\nfor parallel optimization.\n\nThis example shows how to start the dask scheduler and spawn\nworkers for *Auto-sklearn* manually within Python. Use this example\nas a starting point to parallelize *Auto-sklearn* across multiple\nmachines. If you want to start everything manually from the command line\nplease see `this example <example_parallel_manual_spawning_cli.html>`_.\nTo run *Auto-sklearn* in parallel on a single machine check out the example\n`Parallel Usage on a single machine <example_parallel_n_jobs.html>`_.\n\nWhen manually passing a dask client to Auto-sklearn, all logic\nmust be guarded by ``if __name__ == \"__main__\":`` statements! We use\nmultiple such statements to properly render this example as a notebook\nand also allow execution via the command line.\n\n## Background\n\nTo run Auto-sklearn distributed on multiple machines we need to set\nup three components:\n\n1. **Auto-sklearn and a dask client**. This will manage all workload, find new\n   configurations to evaluate and submit jobs via a dask client. As this\n   runs Bayesian optimization it should be executed on its own CPU.\n2. **The dask workers**. They will do the actual work of running machine\n   learning algorithms and require their own CPU each.\n3. **The scheduler**. It manages the communication between the dask client\n   and the different dask workers. As the client and all workers connect\n   to the scheduler it must be started first. This is a light-weight job\n   and does not require its own CPU.\n\nWe will now start these three components in reverse order: scheduler,\nworkers and client. Also, in a real setup, the scheduler and the workers should\nbe started from the command line and not from within a Python file via\nthe ``subprocess`` module as done here (for the sake of having a self-contained\nexample).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import asyncio\nimport multiprocessing\nimport time\n\nimport dask\nimport dask.distributed\nimport sklearn.datasets\nimport sklearn.metrics\n\nfrom autosklearn.classification import AutoSklearnClassifier\nfrom autosklearn.constants import MULTICLASS_CLASSIFICATION\n\ntmp_folder = '/tmp/autosklearn_parallel_2_example_tmp'\noutput_folder = '/tmp/autosklearn_parallel_2_example_out'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define function to start worker\n\nDefine the function to start a dask worker from python. This\nis a bit cumbersome and should ideally be done from the command line.\nWe do it here only for illustrational purpose.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Check the dask docs at\n# https://docs.dask.org/en/latest/setup/python-advanced.html for further\n# information.\n\ndef start_python_worker(scheduler_address):\n    dask.config.set({'distributed.worker.daemon': False})\n\n    async def do_work():\n        async with dask.distributed.Nanny(\n            scheduler_ip=scheduler_address,\n            nthreads=1,\n            lifetime=35,  # automatically shut down the worker so this loop ends\n            memory_limit=0,  # Disable memory management as it is done by Auto-sklearn itself\n        ) as worker:\n            await worker.finished()\n\n    asyncio.get_event_loop().run_until_complete(do_work())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Start Auto-sklearn\n\nWe are now ready to start *auto-sklearn and all dask related processes.\n\nTo use auto-sklearn in parallel we must guard the code with\n``if __name__ == '__main__'``. We then start a dask cluster as a context,\nwhich means that it is automatically stopped once all computation is done.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n    X, y = sklearn.datasets.load_breast_cancer(return_X_y=True)\n    X_train, X_test, y_train, y_test = \\\n        sklearn.model_selection.train_test_split(X, y, random_state=1)\n\n    # 1. Create a dask scheduler (LocalCluster)\n    with dask.distributed.LocalCluster(\n        n_workers=0, processes=True, threads_per_worker=1,\n    ) as cluster:\n\n        # 2. Start the workers\n        # now we start the two workers, one from within Python, the other\n        # via the command line.\n        worker_processes = []\n        for _ in range(2):\n            process_python_worker = multiprocessing.Process(\n                target=start_python_worker,\n                args=(cluster.scheduler_address, ),\n            )\n            process_python_worker.start()\n            worker_processes.append(process_python_worker)\n\n        # Wait a second for workers to become available\n        time.sleep(1)\n\n        # 3. Start the client\n        with dask.distributed.Client(address=cluster.scheduler_address) as client:\n            automl = AutoSklearnClassifier(\n                time_left_for_this_task=30,\n                per_run_time_limit=10,\n                memory_limit=1024,\n                tmp_folder=tmp_folder,\n                output_folder=output_folder,\n                seed=777,\n                # n_jobs is ignored internally as we pass a dask client.\n                n_jobs=1,\n                # Pass a dask client which connects to the previously constructed cluster.\n                dask_client=client,\n            )\n            automl.fit(X_train, y_train)\n\n            automl.fit_ensemble(\n                y_train,\n                task=MULTICLASS_CLASSIFICATION,\n                dataset_name='digits',\n                ensemble_size=20,\n                ensemble_nbest=50,\n            )\n\n        predictions = automl.predict(X_test)\n        print(automl.sprint_statistics())\n        print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, predictions))\n\n        # Wait until all workers are closed\n        for process in worker_processes:\n            process_python_worker.join()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}